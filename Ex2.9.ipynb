{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "treated-formula",
   "metadata": {},
   "source": [
    "# A parameter study of various bandit algorithms in nonstationary case\n",
    "Exercise 2.9 (programming) Make a figure analogous to Figure 2.6 for the non-stationary case outlined in Exercise 2.5. Include the constant-step-size ε-greedy algorithm with α = 0.1. Use runs of 200,000 steps and, as a performance measure for each algorithm and parameter setting, use the average reward over the last 100,000 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-conditions",
   "metadata": {},
   "source": [
    "![avactor](Fig2_6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "obvious-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import normal, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "absent-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from Ex2.5\n",
    "np.random.seed(2021)\n",
    "def generate_r(num_of_arms = 10, centre = 0, sd = 0):\n",
    "    return normal(centre, sd, num_of_arms)\n",
    "\n",
    "def update_r(r, step_centre = 0, step_sd = 0.01):\n",
    "    step = normal(step_centre, step_sd, r.shape)\n",
    "    return np.add(r, step)\n",
    "\n",
    "def get_r(r, sd = 1):\n",
    "    return normal(r, sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "painful-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rl(object):\n",
    "    \n",
    "    def __init__(self, num_of_arms = 10, init_q = None):\n",
    "        self.num_of_arms = num_of_arms\n",
    "        self.q = np.zeros(num_of_arms)\n",
    "        if(init_q != None):\n",
    "            self.q = init_q\n",
    "        self.actions = []\n",
    "        self.count = np.zeros(num_of_arms)\n",
    "        self.rewards = []\n",
    "        \n",
    "    def step(self, r):\n",
    "        action = choose_action()\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(r[action])\n",
    "        self.count[action] += 1\n",
    "        update_q(r, action)\n",
    "    \n",
    "    def choose_action(self):\n",
    "        return np.random.randint(0, self.num_of_arms)\n",
    "    \n",
    "    def update_q(self, r, action):\n",
    "        self.q[action] += (1/self.count[action]) * (r[action] - self.q[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "clear-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon_greedy\n",
    "class epsilon_greedy(rl):\n",
    "    def __init__(self, num_of_arms = 10, init_q = None, epsilon = 0.1):\n",
    "        rl.__init__(self, num_of_arms, init_q)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def choose_action(self):\n",
    "        if unifrom() > epsilon:\n",
    "            return np.argmax(self.q)\n",
    "        return rl.choose_action(self)\n",
    "# UCB\n",
    "class UCB(rl):\n",
    "    def __init__(self, num_of_arms = 10, init_q = None, c = 1):\n",
    "        rl.__init__(self, num_of_arms, init_q)\n",
    "        self.c = c\n",
    "        \n",
    "    def choose_action(self, t):\n",
    "        if len(self.actions) < num_of_arms:\n",
    "            return len(self.actions)\n",
    "        return np.argmax(self.q + self.c * np.sqrt(np.ln(t) / count))\n",
    "    \n",
    "    def step(self, r, t):\n",
    "        action = choose_action(t)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(r[action])\n",
    "        self.count[action] += 1\n",
    "        update_q(r, action)\n",
    "    \n",
    "    \n",
    "#grandient_bandit\n",
    "def softmax(H):\n",
    "    H_ = np.exp(H)\n",
    "    return H_ / np.sum(H_)\n",
    "\n",
    "class gradient_bandit(rl):\n",
    "    def __init__(self, num_of_arms = 10, init_q = None, alpha = 0.25):\n",
    "        rl.__init__(self, num_of_arms, init_q)\n",
    "        self.alpha = alpha\n",
    "        self.R = 0\n",
    "        self.T = 0\n",
    "        \n",
    "    def choose_action(self, p):\n",
    "        return np.random.choice(self.num_of_arms, p = p)\n",
    "        \n",
    "    \n",
    "    def step(self, r):\n",
    "        p = softmax(self.q)\n",
    "        action = choose_action(p)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(r[action])\n",
    "        self.count[action] += 1\n",
    "        \n",
    "#         update average reward\n",
    "        self.T += 1\n",
    "        self.R += 1/T * r[action]\n",
    "        \n",
    "        update_q(r, action, p)\n",
    "    \n",
    "    def update_q(self, r, action, p):\n",
    "        HAt = self.q[action] + self.alpha * (r[action] - self.R) * ( 1 - p[action])\n",
    "        self.q = self.q - self.alpha * (r[action] - self.R) * p\n",
    "        self.q[action] = HAt\n",
    "        \n",
    "#greedyWithInit  alpha = 0.1\n",
    "class greedyWithInit(rl):\n",
    "    def __init__(self, num_of_arms = 10, init_q = None, Q0 = 1, alpha = 0.1):\n",
    "        rl.__init__(self, num_of_arms, init_q)\n",
    "        self.q = np.full(num_of_arms, Q0)\n",
    "    \n",
    "    def choose_action():\n",
    "        return np.argmax(self.q)\n",
    "    \n",
    "    def update_q(self, r, action):\n",
    "        self.q[action] += self.alpha * (r[action] - self.q[action])\n",
    "\n",
    "#constant_step_size epsilon greedy\n",
    "class css_epsilon_greedy(epsilon_greedy):\n",
    "    def __init__(self, num_of_arms = 10, init_q = None, epsilon = 0.1, alpha = 0.1):\n",
    "        epsilon_greedy.__init__(self, num_of_arms, init_q, epsilon)\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def update_q(self, r, action):\n",
    "        self.q[action] += self.alpha * (r[action] - self.q[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "renewable-stretch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09003057, 0.24472847, 0.66524096])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "super-saint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "preceding-samoa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-retirement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
